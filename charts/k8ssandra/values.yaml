cassandra:
  # -- The Cassandra version to use. The supported versions include the following:
  #    - 3.11.7
  #    - 3.11.8
  #    - 3.11.9
  #    - 3.11.10
  #
  version: "3.11.10"

  # -- Specifies the image to use for a particular Cassandra version. Exercise
  # care and caution with changing these values! cass-operator is not designed to work with
  # arbitrary Cassandra images. It expects the cassandra container to be running
  # management-api images. If you do want to change one of these mappings, the new value
  # should be a management-api image.
  versionImageMap:
    3.11.7: datastax/cassandra-mgmtapi-3_11_7:v0.1.20
    3.11.8: datastax/cassandra-mgmtapi-3_11_8:v0.1.20
    3.11.9: datastax/cassandra-mgmtapi-3_11_9:v0.1.20
    3.11.10: datastax/cassandra-mgmtapi-3_11_10:v0.1.20

  # -- Overrides the default image mappings. This is intended for advanced use cases
  # like development or testing. By default the Cassandra version has to be one that is in
  # versionImageMap. Template rendering will fail if the version is not in the map. When
  # you set the image directly, the version mapping check is skipped. Note that you are
  # still constrained to the versions supported by cass-operator.
  #
  # image:

  # -- Cluster name defaults to release name when not specified.
  clusterName: ""

  # -- Authentication and authorization related settings.
  auth:
    # -- Enables or disables authentication and authorization
    enabled: true

    # -- If neither superuser.secret nor superuser.username are set, then
    # k8ssandra falls back to cass-operator's default behavior. cass-operator
    # will create a default superuser and generate a secret whose name is of the
    # form {clusterName}-superuser. If superuser.secret is set, k8ssandra will
    # configure the CassandraDatacenter to use that secret. If superuser.secret
    # is not set and if superuser.username is set, k8ssandra will generate a
    # secret with the username and a random 20 character password. The secret
    # generated by k8ssandra will have a name of the form
    # {clusterName}-superuser.
    superuser:
      secret: ""
      username: ""

    # -- Cache entries validity period in milliseconds. cassandra.yaml has
    # settings for roles, permissions, and credentials caches. This property
    # will configure the validity period for all three.
    cacheValidityPeriodMillis: 3600000

    # -- Cache entries update period in milliseconds. cassandra.yaml has
    # settings for roles, permissions, and credentials caches. This property
    # will configure the update interval for all three.
    cacheUpdateIntervalMillis: 3600000

  # -- The name of the Service Account used to run the cassandra pods.
  serviceAccountName: default

  cassandraLibDirVolume:
    # -- Storage class for persistent volume claims (PVCs) used by the
    # underlying cassandra pods. Depending on your Kubernetes distribution this
    # may be named "standard", "hostpath", or "localpath". Run `kubectl get
    # storageclass` to identify what is available in your environment.
    storageClass: standard

    # -- Size of the provisioned persistent volume per node. It is recommended
    # to keep the total amount of data per node to approximately 1 TB. With room
    # for compactions this value should max out at ~2 TB. This recommendation is
    # highly dependent on data model and compaction strategies in use. Consider
    # testing with your data model to find an optimal value for your usecase.
    size: 5Gi

  # -- Permits running multiple Cassandra pods per Kubernetes worker. If enabled
  # resources.limits and resources.requests **must** be defined.
  allowMultipleNodesPerWorker: false

  # -- Cluster-level heap configuration
  heap:
   size:
   newGenSize:

  # -- Resource requests for each Cassandra pod.
  resources: {}


  datacenters:

  # Note the odd spacing below is to assist with docs generation
  -
    # -- Name of the datacenter
    name: dc1

    # -- Number of nodes within the datacenter. This value should, at a minimum,
    # match the number of racks and be no less than 3 for non-development
    # environments.
    size: 1

    # -- Specifies the racks for the data center, if unset the datacenter will
    # be composed of a single rack named `default`. The number of racks should
    # equal the replication factor of your application keyspaces. Cassandra will
    # ensure that replicas are spread across racks versus having multiple
    # replicas within the same rack. For example, let's say we are using RF = 3
    # with a 9 node cluster and 3 racks (and 3 nodes per rack). There will be
    # one replica of the dataset spread across each rack.
    racks:
    -
      # -- Identifier for the rack, this may align with the labels used to
      # control where resources are deployed for this rack. For example, if a
      # rack is limited to a single availability zone the identifier may be the
      # name of that AZ (eg us-east-1a).
      name: default

      # -- an optional set of labels that are used to pin Cassandra pods to
      # specific k8s worker nodes via affinity rules. See
      # https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/
      # for background on using affinity rules.
      #
      # topology.kubernetes.io/zone is a well-known k8s label used by cloud
      # providers to indicate the failure zone in which a k8s worker node is
      # running. The following example illustrates how you can pin racks to
      # specific failure zones.
      affinityLabels: {}

    # -- Optional datacenter-level heap setting, overrides cluster-level setting
    # `cassandra.heap`
    heap:
      size:
      newGenSize:

stargate:
  # -- Enable Stargate resources as part of this release
  enabled: false

  # -- Number of Stargate instances to deploy. This value may be scaled independently of
  # Cassandra cluster nodes. Each instance handles API and coordination tasks
  # for inbound queries.
  replicas: 1

  # -- Sets the Stargate container image. If left blank (recommended), k8ssandra
  # will derive an appropriate image based on your cluster version.
  image:

  # -- Sets the imagePullPolicy used by the Stargate pods
  imagePullPolicy: IfNotPresent

  # -- Sets the heap size Stargate will use in megabytes. Memory request and
  # limit for the pod will be set to this value x2 and x4, respectively.
  heapMB: 256

  # -- Sets the CPU request for the Stargate pod in millicores.
  cpuReqMillicores: 200

  # -- Sets the CPU limit for the Stargate pod in millicores.
  cpuLimMillicores: 1000

  # -- Configures the Cassandra user used by Stargate when authentication is
  # enabled. If neither `cassandraUser.secret` nor `casandraUser.username` are
  # set, then a Cassandra user and a secret will be created. The username will
  # be `stargate`. The secret name will be of the form `{clusterName}-stargate`. The
  # password will be a random 20 character password. If `cassandraUser.secret`
  # is set, then the Cassandra user will be created from the contents of the
  # secret. If `cassandraUser.secret` is not set and if
  # `cassandraUser.username` is set, a secret will be generated using the
  # specified username. The password will be generated as previously
  # described.
  cassandraUser:
    secret: ""
    username: ""

repair:
  reaper:
    # -- When enabled, Reaper automatically sets up repair schedules for all
    # non-system keypsaces. Repear monitors the cluster so that as keyspaces are
    # added or removed repair schedules will be added or removed respectively.
    autoschedule: false

    # -- Enable Reaper resources as part of this release. Note that Reaper uses
    # Cassandra's JMX APIs to perform repairs. When Reaper is enabled, Cassandra
    # will also be configured to allow remote JMX access. JMX authentication
    # will be configured in Cassandra with credentials only created for Reaper
    # in order to limit access.
    enabled: true

    # Configures the Reaper container image to use.
    image:
      # -- Specifies the container repository for cassandra-reaper
      repository: docker.io/thelastpickle/cassandra-reaper
      # -- Tag of an image within the specified repository
      tag: 2.1.3

    # -- Configures the Cassandra user used by Reaper when authentication is
    # enabled. If neither cassandraUser.secret nor casandraUser.username are
    # set, then a Cassandra user and a secret with the user's credentials will
    # be created. The username will be reaper. The secret name will be of the
    # form {clusterName}-reaper. The password will be a random 20 character
    # password. If cassandraUser.secret is set, then the Cassandra user will be
    # created from the contents of the secret. If cassandraUser.secret is not
    # set and if cassandraUser.username is set, a secret will be generated using
    # the specified username. The password will be generated as previously
    # described.
    cassandraUser:
      secret: ""
      username: ""

    # -- Configures JMX access to the Cassandra cluster. Reaper requires remote
    # JMX access to perform repairs. The Cassandra cluster will be configured
    # with remote JMX access enabled when Reaper is deployed. The JMX access
    # will be configured to use authentication.
    jmx:
      # -- Username that Reaper will use for JMX access. If left blank a random,
      # alphanumeric string will be generated.
      username: ""

      # -- Password that Reaper will use for JMX access. If left blank a random,
      # alphanumeric string will be generated.
      password: ""

backupRestore:
  medusa:
    # -- Enable Medusa resources as part of this release. If enabled,
    # `bucketName` and `bucketSecret` **must** be defined.
    enabled: false

    # Configures the Medusa image which is built from
    # https://github.com/thelastpickle/cassandra-medusa/tree/master/k8s.
    image:
      # -- Specifies the container repository for Medusa
      repository: docker.io/k8ssandra/medusa
      # -- Tag of an image within the specified repository
      tag: 6ab6a55541e9
      # -- The image pull policy
      pullPolicy: IfNotPresent

    # -- Configures the Cassandra user used by Medusa when authentication is
    # enabled. If neither `cassandraUser.secret` nor `casandraUser.username` are
    # set, then a Cassandra user and a secret will be created. The username will
    # be medusa. The secret name will be of the form {clusterName}-medusa. The
    # password will be a random 20 character password. If `cassandraUser.secret`
    # is set, then the Cassandra user will be created from the contents of the
    # secret. If `cassandraUser.secret` is not set and if
    # `cassandraUser.username` is set, a secret will be generated using the
    # specified username. The password will be generated as previously
    # described.
    cassandraUser:
      secret: ""
      username: ""

    # -- Enables usage of a bucket across multiple clusters.
    multiTenant: false

    # -- API interface used by the object store. Supported values include `s3`
    # and `gcs`
    storage: s3

    # -- Name of the remote storage bucket where backups will be stored.
    bucketName: ""

    # -- Name of the Kubernetes `Secret` that stores the key file for the
    # storage provider's API
    bucketSecret: ""

# Support for accessing the various web-interfaces via K8s ingress controllers.
# Depending on the ingress available in your cluster enable the appropriate
# section.
ingress:
  traefik:
    # -- Enable Traefik custom resources as part of this release.
    enabled: false

    # Repair service
    repair:
      # -- Enables Reaper Traefik ingress definitions. Note this will **only**
      # work if `ingress.traefik.enabled` is also `true`
      enabled: true

      # -- Traefik entrypoints where traffic is sourced.
      entrypoints:
        - web

      # -- Hostname Traefik should use for routing requests to the repair UI. If
      # using a local deployment consider leveraging dynamic DNS services like
      # xip.io. Example: `repair.127.0.0.1.xip.io` will return `127.0.0.1` for
      # DNS requests routing requests to your local machine.
      host: repair.k8ssandra.cluster.local

    # Cassandra native transport ingress support
    cassandra:
      # -- Enables Cassandra Traefik ingress definitions. Note this will
      # **only** work if `ingress.traefik.enabled` is also `true`. Additionally,
      # this is mutually exclusive with
      # ingress.traefik.stargate.cassandra.enabled
      enabled: true

      # -- Traefik entrypoints where traffic is sourced.
      entrypoints:
        - cassandra

    # Stargate ingress support
    stargate:
      # -- Enables Stargate Traefik ingress definitions. Note this will **only**
      # work if `stargate.enabled` and `ingress.traefik.enabled` are also `true`. Additionally, when
      # enabled the authentication API (port 8081) is enabled automatically.
      enabled: true

      # -- Hostname Traefik should use for routing requests to Stargate. If
      # using a local deployment consider leveraging dynamic DNS services like
      # xip.io. Example: `stargate.127.0.0.1.xip.io` will return `127.0.0.1` for
      # DNS requests routing requests to your local machine.
      host: "*"

      # Ingress for each Stargate API can be enabled/disabled independently
      graphql:
        # -- Enables ingress resources for Stargate GraphQL API. Note this will
        # **only** work if `stargate.enabled`, `ingress.traefik.enabled`, and
        # `ingress.traefik.stargate.enabled` are also `true`
        enabled: true

        playground:
          # -- Enables GraphQL playground interface ingress.
          enabled: false

      rest:
        # -- Enables ingress resources for Stargate REST API. Note this will
        # **only** work if `stargate.enabled`, `ingress.traefik.enabled`, and
        # `ingress.traefik.stargate.enabled` are also `true`
        enabled: true

      cassandra:
        # -- Enables Traefik ingress resources for Stargate Cassandra API. Note
        # this will **only** work if `stargate.enabled`,
        # `ingress.traefik.enabled`, and `ingress.traefik.stargate.enabled` are
        # also `true`. Additionally, this is mutually exclusive with
        # ingress.traefik.cassandra.enabled
        enabled: false
        entrypoints:
          - cassandra

monitoring:
  grafana:
    # -- Enables the creation of configmaps containing Grafana dashboards. If
    # leveraging the kube prometheus stack sub-chart this value should be
    # `true`.
    provision_dashboards: true

  prometheus:
    # -- Enabes the creation of Prometheus Operator ServiceMonitor custom
    # resources. If you are not using the kube prometheus stack sub-chart or do
    # not have the ServiceMonitor CRD installed on your cluster, set this value
    # to `false`.
    provision_service_monitors: true

cleaner:
  image: k8ssandra/k8ssandra-cleaner:618b8ff9d368

cass-operator:
  # -- Enables the cass-operator as part of this release. If this setting is
  # disabled no Cassandra resources will be deployed.
  enabled: true

reaper-operator:
  # -- Enables the reaper-operator as part of this release. If this setting is
  # disabled no repair resources will be deployed.
  enabled: true

# Configuration values for the kube-prometheus-stack chart. Not all values are
# provided here for an exhaustive list see:
# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
kube-prometheus-stack:
  # -- Controls whether the kube-prometheus-stack chart is used at all.
  # Disabling this parameter prevents all monitoring components from being
  # installed.
  enabled: true

  # Disable default service monitors
  coreDns:
    enabled: false
  kubeApiServer:
    enabled: false
  kubeControllerManager:
    enabled: false
  kubeDns:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeProxy:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeStateMetrics:
    enabled: false
  kubelet:
    enabled: false
  nodeExporter:
    enabled: false

  alertmanager:
    # Disabled for now while we build out a set of default alerts
    enabled: false
    serviceMonitor:
      selfMonitor: false

  prometheusOperator:
    # Installs the Prometheus Operator, omitting this parameter will result in
    # resources not being deployed.
    enabled: true

    # -- Locks Prometheus operator to this namespace. Changing this setting may
    # result in a non-namespace scoped deployment.
    namespaces:
      releaseNamespace: true
      additional: []

    # -- Monitoring of prometheus operator
    serviceMonitor:
      selfMonitor: false

  prometheus:
    # -- Provisions an instance of Prometheus as part of this release
    enabled: true

    # -- Allows for tweaking of the Prometheus installation's configuration.
    # Common parameters include `externalUrl: http://localhost:9090/prometheus`
    # and `routePrefix: /prometheus` for running Prometheus resources under a
    # specific path (`/prometheus` in this example).
    prometheusSpec:
      # -- Prefixes all Prometheus routes with the specified value. It is useful
      # for ingresses which do not rewrite URLs.
      routePrefix: /

      # -- An external URL at which Prometheus will be reachable.
      externalUrl: ""

    ingress:
      # -- Enable templating of ingress resources for external prometheus
      # traffic
      enabled: false
      # -- Path-based routing rules, `/prometheus` is possible if the
      # appropriate changes are made to `prometheusSpec`
      paths: []

    serviceMonitor:
      # Disable monitoring the Prometheus instance
      selfMonitor: false

  grafana:
    # -- Provisions an instance of Grafana and wires it up with a DataSource
    # referencing this Prometheus installation
    enabled: true
    ingress:
      # -- Generates ingress resources for the Grafana instance
      enabled: false

      # -- Path-based routing rules, '/grafana' is possible if appropriate
      # changes are made to `grafana.ini`
      path:

    # -- Username for accessing the provisioned Grafana instance
    adminUser: admin

    # -- Password for accessing the provisioned Grafana instance
    adminPassword: secret

    serviceMonitor:
      # -- Whether the Grafana instance should be monitored
      selfMonitor: false

    # -- Default dashboard installation
    defaultDashboardsEnabled: false

    # -- Additional plugins to be installed during Grafana startup,
    # `grafana-polystat-panel` is used by the default Cassandra dashboards.
    plugins:
      - grafana-polystat-panel

    # -- Customization of the Grafana instance. To listen for Grafana traffic
    # under a different url set `server.root_url: http://localhost:3000/grafana`
    # and `serve_from_sub_path: true`.
    grafana.ini: {}
