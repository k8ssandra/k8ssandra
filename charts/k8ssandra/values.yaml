cassandra:
  version: "3.11.9"
  clusterName: k8ssandra
  # auth is authentication and authorization related settings.
  auth:
    # enabled enables or disables authentication and authorization
    enabled: true

    # If neither superuser.secret nor superuser.username are set, then
    # k8ssandra falls back to cass-operator's default behavior. cass-operator
    # will create a default superuser and generate a secret whose name is of
    # the form {clusterName}-superuser. If superuser.secret is set, k8ssandra
    # will configure the CassandraDatacenter to use that secret. If
    # superuser.secret is not set and if superuser.username is set, k8ssandra
    # will generate a secret with the username and a random 20 character
    # password. The secret generated by k8ssandra will have a name of the
    # form {clusterName}-superuser.
    superuser:
      secret: ""
      username: ""

    # cacheValidityPeriodMillis specifies in milliseconds how long cache entries are
    # valid. cassandra.yaml has settings for roles, permissions, and credentials caches.
    # This property will configure the validity period for all three.
    cacheValidityPeriodMillis: 3600000
    # cacheUpdateIntervalMillis specifies in milliseconds the interval or frequency that
    # cache entries are updated. cassandra.yaml has settings for roles, permissions, and
    # credentials caches. This property will configure the update interval for all three.
    cacheUpdateIntervalMillis: 3600000

  cassandraLibDirVolume:
    storageClass: standard
    size: 5Gi
  # If enabled, resources.limits and resources.requests must be defined
  allowMultipleNodesPerWorker: false

  # -- Optional cluster-level heap setting
  #heap:
  #  size: 800M
  #  newGenSize: 400M

  resources: {}

  datacenters:
  - name: dc1
    size: 1
    # racks is an array that specifies the racks for the data center. If not set the data
    # center will be configured with a single rack named default.
    # racks:
    #
    # The number of racks should equal the replication factor of your application
    # keyspaces. Cassandra will ensure that replicas are spread across racks versus having
    # multiple replicas within the same rack. For example, let's say we are using RF = 3
    # with a 9 node cluster and 3 racks (and 3 nodes per rack). There will be one replica
    # per rack.
    #
    # Each element in the racks array consists of two properties - name and affinityLabels.
    # name specifies the rack name and is required. affinityLabels are an optional set of
    # labels that are used to pin Cassandra pods to specific k8s worker nodes via affinity
    # rules. See https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/
    # for background on using affinity rules.
    #
    # topology.kubernetes.io/zone is a well-known k8s label used by cloud providers to
    # indicate the failure zone in which a k8s worker node is running. The following
    # example illustrates how you can pin racks to specific failure zones.
    #
    # racks:
    # - name: r1
    #   affinityLabels:
    #     topology.kubernetes.io/zone: us-east1-b
    # - name: r2
    #   affinityLabels:
    #     topology.kubernetes.io/zone: us-east1-a
    # - name: r3
    #   affinityLabels:
    #     topology.kubernetes.io/zone: us-east1-c

    # -- Optional dc-level heap setting, overrides cluster-level
    #heap:
    #size: 800M
    #newGenSize: 400M


stargate:
  # configuration regarding the Stargate API
  enabled: false
  replicas: 1
  clusterVersion: "3.11" # this is the only supported value for now
  containerImage: "auto" # chooses image based on clusterVersion

# Configuration regarding configuration of repair services
repair:
  # Reaper repair service and UI
  reaper:
    autoschedule: false
    enabled: true

    # cassandraUser configures the Cassandra user used by Reaper when authentication is
    # enabled. If neither cassandraUser.secret nor casandraUser.username are set, then a
    # Cassandra user and a secret with the user's credentials will be created. The username
    # will be reaper. The secret name will be of the form {cluserName}-reaper. The password
    # will be a random 20 character password. If cassandraUser.secret is set, then the
    # Cassandra user will be created from the contents of the secret. If
    # cassandraUser.secret is not set and if cassandraUser.username is set, a secret will
    # be generated using the specified username. The password will be generated as
    # previously described.
    cassandraUser:
      secret: ""
      username: ""
    jmx:
      username: ""
      password: ""

backupRestore:
  medusa:
    # If enabled, bucketName and bucketSecret must be defined.
    enabled: false
    # image The Medusa image which is built from https://github.com/thelastpickle/cassandra-medusa/tree/master/k8s.
    image: k8ssandra/medusa:c5fefc4a3b4c

    # Set to true to use the same bucket across multiple clusters.
    multiTenant: false

    # Accepted values are s3 and gcs
    storage: s3

    # Must be set to the name of the remote storage bucket where backups will be stored.
    bucketName: ""

    # Must be set and specify the name of the secret that stores the key file for Google
    # Cloud or AWS
    bucketSecret: ""

# Support for accessing the various web-interfaces via K8s ingress controllers.
# Depending on the ingress available in your cluster enable the appropriate
# section.
ingress:
  traefik:
    # Set to `true` to enable the templating of Traefik ingress custom resources
    enabled: false

    # Repair service
    repair:
      # Note this will **only** work if `ingress.traefik.enabled` is also `true`
      enabled: true

      # Name of the Traefik entrypoints where we want to source traffic.
      entrypoints:
        - web

      # Hostname Traefik should use for matching requests.
      host: repair.k8ssandra.cluster.local

    # Cassandra native transport ingress support
    cassandra:
      # Note this will **only** work if `ingress.traefik.enabled` is also `true`
      # Note this is mutually exclusive with ingress.traefik.stargate.cassandra.enabled
      enabled: true

      # Name of the Traefik entrypoints for source traffic
      entrypoints:
        - cassandra

    # Stargate ingress support
    stargate:
      # Note this will **only** work if `ingress.traefik.enabled` is also `true`
      # Note when enabled, the authentication API (port 8081) is enabled automatically.
      enabled: false

      # Hostname Traefik should use for matching Stargate requests.
      host: localhost

      # ingress for each Stargate API can be enabled/disabled independently
      graphql:
        # Note this will **only** work if `stargate.enabled`, `ingress.traefik.enabled`, and `ingress.traefik.stargate.enabled` are also `true`
        enabled: true
        playground:
          enabled: false
      rest:
        # Note this will **only** work if `stargate.enabled`, `ingress.traefik.enabled`, and `ingress.traefik.stargate.enabled` are also `true`
        enabled: true
      cassandra:
        # Note this will **only** work if `stargate.enabled`, `ingress.traefik.enabled`, and `ingress.traefik.stargate.enabled` are also `true`
        # Note this is mutually exclusive with ingress.traefik.cassandra.enabled
        enabled: false
        entrypoints:
          - cassandra

monitoring:
  grafana:
    provision_dashboards: true

  prometheus:
    provision_service_monitors: true

cleaner:
  image: k8ssandra/k8ssandra-cleaner:618b8ff9d368

cass-operator:
  enabled: true

reaper-operator:
  enabled: true

# Configuration values for the kube-prometheus-stack chart. Not all values are
# provided here for an exhaustive list see:
# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
kube-prometheus-stack:
  # Controls whether this chart is installed at all.
  enabled: true

  # Disable default service monitors
  coreDns:
    enabled: false
  kubeApiServer:
    enabled: false
  kubeControllerManager:
    enabled: false
  kubeDns:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeProxy:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeStateMetrics:
    enabled: false
  kubelet:
    enabled: false
  nodeExporter:
    enabled: false

  alertmanager:
    # Disabled for now while we build out a set of default alert
    enabled: false
    serviceMonitor:
      selfMonitor: false

  prometheusOperator:
    # Installs the Prometheus Operator
    enabled: true

    # Locks Prometheus operator to this namespace
    namespaces:
      releaseNamespace: true
      additional: []

    # Disable monitoring of operator
    serviceMonitor:
      selfMonitor: false

  prometheus:
    # Provisions an instance of Prometheus as part of this release
    enabled: true
    prometheusSpec:
      # Uncomment to listen for Prometheus traffic under a route prefix
      # externalUrl: http://localhost:9090/prometheus
      # routePrefix: /prometheus

    # Enable templating of ingress resources for external prometheus traffic
    ingress:
      enabled: false
      paths: []
        # Uncomment to listen for Prometheus traffic under a route prefix
        # - /prometheus

    # Disable monitoring the Prometheus instance
    serviceMonitor:
      selfMonitor: false

    # Disable default service monitors
    # additionalServiceMonitors: []

  grafana:
    # Provisions an instance of Grafana and wires it up with a DataSource
    # referencing this Prometheus installation
    enabled: true
    ingress:
      enabled: false
      # Uncomment to listen for Grafana traffic under a route prefix
      # path: /grafana

    # Credentials for accessing the provisioned Grafana
    adminUser: admin
    adminPassword: admin

    # Disable monitoring Grafana
    serviceMonitor:
      selfMonitor: false

    # Disable default dashboards
    defaultDashboardsEnabled: false

    # Additional plugins to be installed during Grafana startup
    plugins:
      # Used by the cassandra-overview dashboard
      - grafana-polystat-panel

    # Uncomment to listen for Grafana traffic under a route prefix
    # grafana.ini:
    #   server:
    #     root_url: http://localhost:3000/grafana
    #     serve_from_sub_path: true
