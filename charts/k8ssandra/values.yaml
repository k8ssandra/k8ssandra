cassandra:
  # -- The Cassandra version to use. The supported versions include the following:
  #    - 3.11.7
  #    - 3.11.8
  #    - 3.11.9
  #    - 3.11.10
  #    - 4.0.0
  version: "3.11.10"

  # -- Specifies the image to use for a particular Cassandra version. Exercise
  # care and caution with changing these values! cass-operator is not designed to work with
  # arbitrary Cassandra images. It expects the cassandra container to be running
  # management-api images. If you do want to change one of these mappings, the new value
  # should be a management-api image.
  versionImageMap:
    3.11.7: datastax/cassandra-mgmtapi-3_11_7:v0.1.22
    3.11.8: datastax/cassandra-mgmtapi-3_11_8:v0.1.22
    3.11.9: datastax/cassandra-mgmtapi-3_11_9:v0.1.22
    3.11.10: datastax/cassandra-mgmtapi-3_11_10:v0.1.22
    4.0.0: datastax/cassandra-mgmtapi-4_0_0:v0.1.22

  # -- Overrides the default image mappings. This is intended for advanced use cases
  # like development or testing. By default the Cassandra version has to be one that is in
  # versionImageMap. Template rendering will fail if the version is not in the map. When
  # you set the image directly, the version mapping check is skipped. Note that you are
  # still constrained to the versions supported by cass-operator.
  #
  # image:

  # -- Cluster name defaults to release name when not specified.
  clusterName: ""

  # -- Authentication and authorization related settings.
  auth:
    # -- Enables or disables authentication and authorization. This also
    # enables/disables JMX authentication. Note that if Reaper is enabled JMX
    # authentication will still be enabled even if auth is disabled here. This
    # is because Reaper requires remote JMX access.
    enabled: true

    # -- Configures the default Cassandra superuser when authentication is
    # enabled. If neither `superuser.secret` nor `superuser.username` are set,
    # then a user and a secret with the user's credentials will be created. The
    # username and secret name will be of the form {clusterName}-superuser. The
    # password will be a random 20 character password. If `superuser.secret` is
    # set, then the Cassandra user will be created from the contents of the
    # secret. If `superuser.secret` is not set and if `superuser.username` is
    # set, a secret will be generated using the specified username. The
    # password will be generated as previously described.
    #
    # JMX credentials will also be created for the superuser. The same
    # username/password that is used here will be used for JMX. If you change
    # the Cassandra superuser credentials through cqlsh for example, the JMX
    # credentials will not be updated. You need to update the credentials via
    # helm upgrade in order for the change to propagate to JMX. This will be
    # fixed in https://github.com/k8ssandra/k8ssandra/issues/323.
    superuser:
      secret: ""
      username: ""

    # -- Cache entries validity period in milliseconds. cassandra.yaml has
    # settings for roles, permissions, and credentials caches. This property
    # will configure the validity period for all three.
    cacheValidityPeriodMillis: 3600000

    # -- Cache entries update period in milliseconds. cassandra.yaml has
    # settings for roles, permissions, and credentials caches. This property
    # will configure the update interval for all three.
    cacheUpdateIntervalMillis: 3600000

  cassandraLibDirVolume:
    # -- Storage class for persistent volume claims (PVCs) used by the
    # underlying cassandra pods. Depending on your Kubernetes distribution this
    # may be named "standard", "hostpath", or "localpath". Run `kubectl get
    # storageclass` to identify what is available in your environment.
    storageClass: standard

    # -- Size of the provisioned persistent volume per node. It is recommended
    # to keep the total amount of data per node to approximately 1 TB. With room
    # for compactions this value should max out at ~2 TB. This recommendation is
    # highly dependent on data model and compaction strategies in use. Consider
    # testing with your data model to find an optimal value for your usecase.
    size: 5Gi

  # -- Permits running multiple Cassandra pods per Kubernetes worker. If enabled
  # resources.limits and resources.requests **must** be defined.
  allowMultipleNodesPerWorker: false

  # -- Optional cluster-level heap configuration, can be overridden at `datacenters` level.
  # Options are commented out for reference.
  heap: {}
   #size:
   #newGenSize:

  # -- Optional cluster-level garbage collection configuration. It can be overridden at
  # the datacenter level.
  gc:
    # -- GC configuration for the CMS collector.
    cms: {}
      # -- Enables the CMS garbage collector
      # enabled: true

      # -- Controls the size of the two survivor spaces in the heap's young
      # generation.
      # survivorRatio: 8

      # -- The number of times an object survives a minor collection before
      # being promoted to the old generation.
      # maxTenuringThreshold: 1

      # -- A major collection starts if the occupancy of the old generation
      # exceeds this percentage.
      # initiatingOccupancyFraction: 75

      # -- The time in milliseconds that CMS threads wait for young GC.
      # waitDuration: 10000

    # -- GC configuration for the G1 collector.
    g1: {}
      # -- Enabled the G1 garbage collector.
      # enabled: true

      # -- Determines the percentage of total garbage collection time that
      # should be spent in the Update RS phase updating any remaining
      # remembered sets. G1 controls the amount of concurrent remembered set
      # updates using this setting.
      # setUpdatingPauseTimePercent: 5

      # -- Sets a target value for desired maximum pause time.
      # maxGcPauseMillis: 500

      # -- Sets the heap occupancy threshold that triggers a marking cycle.
      # initiatingHeapOccupancyPercent: 70

      # -- Set the number of stop the world (STW) worker threads.
      # parallelGcThreads: 16

      # -- Set the number of stop the world (STW) worker threads.
      # concurrentGcThreads: 16

  # -- Resource requests for each Cassandra pod.
  resources: {}

  datacenters:

  # Note the odd spacing below is to assist with docs generation
  -
    # -- Name of the datacenter
    name: dc1

    # -- Number of nodes within the datacenter. This value should, at a minimum,
    # match the number of racks and be no less than 3 for non-development
    # environments.
    size: 1

    # -- Number of tokens within the datacenter. If not defined, the default values will be
    # used, which are 256 tokens for 3.11 releases and 16 tokens for 4.0 releases.
    # num_tokens: 16

    # -- Specifies the racks for the data center, if unset the datacenter will
    # be composed of a single rack named `default`. The number of racks should
    # equal the replication factor of your application keyspaces. Cassandra will
    # ensure that replicas are spread across racks versus having multiple
    # replicas within the same rack. For example, let's say we are using RF = 3
    # with a 9 node cluster and 3 racks (and 3 nodes per rack). There will be
    # one replica of the dataset spread across each rack.
    racks:
    -
      # -- Identifier for the rack, this may align with the labels used to
      # control where resources are deployed for this rack. For example, if a
      # rack is limited to a single availability zone the identifier may be the
      # name of that AZ (eg us-east-1a).
      name: default

      # -- an optional set of labels that are used to pin Cassandra pods to
      # specific k8s worker nodes via affinity rules. See
      # https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/
      # for background on using affinity rules.
      #
      # topology.kubernetes.io/zone is a well-known k8s label used by cloud
      # providers to indicate the failure zone in which a k8s worker node is
      # running. The following example illustrates how you can pin racks to
      # specific failure zones.
      affinityLabels: {}

    # -- Optional datacenter-level heap setting, overrides cluster-level setting
    # `cassandra.heap`.  Options are commented out for reference.
    heap: {}
      #size:
      #newGenSize:

    # -- Optional datacenter-level garbage collection configuration.
    gc:
      # -- Optional GC configuration for the CMS collector
      cms: {}
        # -- Enables the CMS garbage collector
        # enabled: true

        # -- Controls the size of the two survivor spaces in the heap's young
        # generation.
        # survivorRatio: 8

        # -- The number of times an object survives a minor collection before
        # being promoted to the old generation.
        # maxTenuringThreshold: 1

        # -- A major collection starts if the occupancy of the old generation
        # exceeds this percentage.
        # initiatingOccupancyFraction: 75

        # -- The time in milliseconds that CMS threads wait for young GC.
        # waitDuration: 10000

      # -- Optional GC configuration for the G1 collector
      g1: {}
        # -- Enabled the G1 garbage collector.
        # enabled: true

        # -- Determines the percentage of total garbage collection time that
        # should be spent in the Update RS phase updating any remaining
        # remembered sets. G1 controls the amount of concurrent remembered set
        # updates using this setting.
        # setUpdatingPauseTimePercent: 5

        # -- Sets a target value for desired maximum pause time.
        # maxGcPauseMillis: 500

        # -- Sets the heap occupancy threshold that triggers a marking cycle.
        # initiatingHeapOccupancyPercent: 70

        # -- Set the number of stop the world (STW) worker threads.
        # parallelGcThreads: 16

        # -- Sets the number of parallel marking threads.
        # concurrentGcThreads: 16

  # Cassandra native transport ingress support
  ingress:
    # -- Enables Cassandra Traefik ingress definitions. Note that
    # this is mutually exclusive with stargate.ingress.cassandra.enabled
    enabled: false

    # -- Determines which TCP-based ingress custom resources to template out. Currently only `traefik` is supported
    method: traefik

    # -- Optional hostname used to match requests. Warning: many native Cassandra clients, notably including cqlsh, initialize their connection by querying for
    # the cluster's contactPoints, and thereafter communicate to the cluster using those names/IPs rather than whatever host was specified to the client. In
    # order for clients to work correctly through ingress with a host filter, this means that the host filter must match the hostnames specified in the
    # contactPoints. This value must be a DNS-resolvable hostname and not an IP address. To avoid this issue, leave this setting blank.
    host:

    traefik:
      # -- Traefik entrypoint where traffic is sourced. See https://doc.traefik.io/traefik/routing/entrypoints/
      entrypoint: cassandra
      # Future parameters
      # tls:
      #   client_auth:
      #     # -- Collection of Secrets containing TLS certificates. KEYS ARE NOT REQUIRED HERE
      #     ca_secrets:
      #       - ca_secret_1
      #   domains:
      #     - example.com
      #   # -- Secret containing the TLS certificate and key for the listener
      #   secretName: tls-secret

stargate:
  # -- Enable Stargate resources as part of this release
  enabled: true

  # -- version of Stargate to deploy. This is used in conjunction with cassandra.version to select the Stargate container image.
  # If stargate.image is set, this value has no effect.
  version: "1.0.9"

  # -- Number of Stargate instances to deploy. This value may be scaled independently of
  # Cassandra cluster nodes. Each instance handles API and coordination tasks
  # for inbound queries.
  replicas: 1

  # -- Sets the Stargate container image. This value must be compatible
  # with the value provided for stargate.clusterVersion. If left blank (recommended),
  # k8ssandra will derive an appropriate image based on cassandra.clusterVersion.
  image:

  # -- Sets the imagePullPolicy used by the Stargate pods
  imagePullPolicy: IfNotPresent

  # -- Sets the heap size Stargate will use in megabytes. Memory request and
  # limit for the pod will be set to this value x2 and x4, respectively.
  heapMB: 256

  # -- Sets the CPU request for the Stargate pod in millicores.
  cpuReqMillicores: 200

  # -- Sets the CPU limit for the Stargate pod in millicores.
  cpuLimMillicores: 1000

  # -- Configures the Cassandra user used by Stargate when authentication is
  # enabled. If neither `cassandraUser.secret` nor `casandraUser.username` are
  # set, then a Cassandra user and a secret will be created. The username will
  # be `stargate`. The secret name will be of the form `{clusterName}-stargate`. The
  # password will be a random 20 character password. If `cassandraUser.secret`
  # is set, then the Cassandra user will be created from the contents of the
  # secret. If `cassandraUser.secret` is not set and if
  # `cassandraUser.username` is set, a secret will be generated using the
  # specified username. The password will be generated as previously
  # described.
  cassandraUser:
    secret: ""
    username: ""

  ingress:
    # -- Optional hostname used to match requests. Warning: many native Cassandra clients, notably including cqlsh, initialize their connection by querying for
    # the cluster's contactPoints, and thereafter communicate to the cluster using those names/IPs rather than whatever host was specified to the client. In
    # order for clients to work correctly through ingress with a host filter, this means that the host filter must match the hostnames specified in the
    # contactPoints. This value must be a DNS-resolvable hostname and not an IP address. To avoid this issue, leave this setting blank, or override it to ""
    # (empty string) for stargate.ingress.cassandra.host. This note does not apply to clients of Stargate's auth, REST, or GraphQL APIs.
    host:

    # -- Enables all Stargate ingresses. Note: This must be true for any Stargate ingress to function.
    enabled: false

    auth:
      # -- Enables Stargate authentication ingress. Note: stargate.ingress.enabled must also be true.
      enabled: true
      # -- Optional hostname used to match requests, overriding stargate.ingress.host if set
      host:
      # tls:
      #   # -- Hostnames to match as part of SNI routing
      #   hosts:
      #     - example.com
      #   # -- Secret containing the TLS certificate and key for encrypting requests to this endpoint
      #   secretName: my-secret

    rest:
      # -- Enables Stargate REST ingress. Note: stargate.ingress.enabled must also be true.
      enabled: true
      # -- Optional hostname used to match requests, overriding stargate.ingress.host if set
      host:
      # tls:
      #   # -- Hostnames to match as part of SNI routing
      #   hosts:
      #     - example.com
      #   # -- Secret containing the TLS certificate and key for encrypting requests to this endpoint
      #   secretName: my-secret

    graphql:
      # -- Enables Stargate GraphQL API ingress. Note: stargate.ingress.enabled must also be true.
      enabled: true
      # -- Optional hostname used to match requests, overriding stargate.ingress.host if set
      host:
      # tls:
      #   # -- Hostnames to match as part of SNI routing
      #   hosts:
      #     - example.com
      #   # -- Secret containing the TLS certificate and key for encrypting requests to this endpoint
      #   secretName: my-secret
      playground:
        # -- Enables GraphQL playground ingress.  Note: stargate.ingress.enabled and stargate.ingress.graphql.enabled must also be true.
        enabled: true

    cassandra:
      # -- Enables C* native protocol ingress with Traefik. Note that this is mutually exclusive with cassandra.ingress.enabled, and stargate.ingress.enabled must also be true.
      enabled: true
      # -- Determines which TCP-based ingress custom resources to template out. Currently only `traefik` is supported
      method: traefik

      # -- Optional hostname used to match requests. Warning: many native Cassandra clients, notably including cqlsh, initialize their connection by querying for
      # the cluster's contactPoints, and thereafter communicate to the cluster using those names/IPs rather than whatever host was specified to the client. In
      # order for clients to work correctly through ingress with a host filter, this means that the host filter must match the hostnames specified in the
      # contactPoints. This value must be a DNS-resolvable hostname and not an IP address. To avoid this issue, leave this setting blank, or if
      # stargate.ingress.host is set, override it here to "" (empty string). This note does not apply to clients of Stargate's auth, REST, or GraphQL APIs.
      host:

      # -- Parameters used by the Traefik IngressRoute custom resource
      traefik:

        # -- Traefik entrypoint where traffic is sourced. See https://doc.traefik.io/traefik/routing/entrypoints/
        entrypoint: cassandra
        # Future parameters
        # tls:
        #   client_auth:
        #     # -- Collection of Secrets containing TLS certificates. KEYS ARE NOT REQUIRED HERE
        #     ca_secrets:
        #       - ca_secret_1
        #   domains:
        #     - example.com
        #   # -- Secret containing the TLS certificate and key for the listener
        #   secretName: tls-secret

reaper:
  # -- When enabled, Reaper automatically sets up repair schedules for all
  # non-system keypsaces. Repear monitors the cluster so that as keyspaces are
  # added or removed repair schedules will be added or removed respectively.
  autoschedule: false

  # -- Enable Reaper resources as part of this release. Note that Reaper uses
  # Cassandra's JMX APIs to perform repairs. When Reaper is enabled, Cassandra
  # will also be configured to allow remote JMX access. JMX authentication
  # will be configured in Cassandra with credentials only created for Reaper
  # in order to limit access.
  enabled: true

  # Configures the Reaper container image to use.
  image:
    # -- Specifies the container repository for cassandra-reaper
    repository: docker.io/thelastpickle/cassandra-reaper
    # -- Tag of an image within the specified repository
    tag: 2.2.1

  # -- Configures the Cassandra user used by Reaper when authentication is
  # enabled. If neither cassandraUser.secret nor casandraUser.username are
  # set, then a Cassandra user and a secret with the user's credentials will
  # be created. The username will be reaper. The secret name will be of the
  # form {clusterName}-reaper. The password will be a random 20 character
  # password. If cassandraUser.secret is set, then the Cassandra user will be
  # created from the contents of the secret. If cassandraUser.secret is not
  # set and if cassandraUser.username is set, a secret will be generated using
  # the specified username. The password will be generated as previously
  # described.
  cassandraUser:
    secret: ""
    username: ""

  # -- Configures JMX access to the Cassandra cluster. Reaper requires remote
  # JMX access to perform repairs. The Cassandra cluster will be configured
  # with remote JMX access enabled when Reaper is deployed. The JMX access
  # will be configured to use authentication.
  #
  # If neither `jmx.secret` nor `jmx.username` are set, then a default user and
  # secret with the user's credentials will be created.
  jmx:
    secret: ""
    # -- Username that Reaper will use for JMX access. If left blank a random,
    # alphanumeric string will be generated.
    username: ""

  ingress:
    # -- Enables Reaper ingress definitions. When enabled, you must specify a value for reaper.ingress.host.
    enabled: false

    # -- Hostname to use for routing requests to the repair UI. If
    # using a local deployment consider leveraging dynamic DNS services like
    # xip.io. Example: `repair.127.0.0.1.xip.io` will return `127.0.0.1` for
    # DNS requests routing requests to your local machine. This is required when reaper.ingress.enabled is true.
    host:

    method: traefik

    traefik:
      # -- Traefik entrypoint where traffic is sourced. See https://doc.traefik.io/traefik/routing/entrypoints/
      entrypoint: web

medusa:
  # -- Enable Medusa resources as part of this release. If enabled,
  # `bucketName` and `storageSecret` **must** be defined.
  enabled: false

  # Configures the Medusa image which is built from
  # https://github.com/thelastpickle/cassandra-medusa/tree/master/k8s.
  image:
    # -- Specifies the container repository for Medusa
    repository: docker.io/k8ssandra/medusa
    # -- Tag of an image within the specified repository
    tag: 0.9.0
    # -- The image pull policy
    pullPolicy: IfNotPresent

  # -- Configures the Cassandra user used by Medusa when authentication is
  # enabled. If neither `cassandraUser.secret` nor `casandraUser.username` are
  # set, then a Cassandra user and a secret will be created. The username will
  # be medusa. The secret name will be of the form {clusterName}-medusa. The
  # password will be a random 20 character password. If `cassandraUser.secret`
  # is set, then the Cassandra user will be created from the contents of the
  # secret. If `cassandraUser.secret` is not set and if
  # `cassandraUser.username` is set, a secret will be generated using the
  # specified username. The password will be generated as previously
  # described.
  cassandraUser:
    secret: ""
    username: ""

  # -- Enables usage of a bucket across multiple clusters.
  multiTenant: false

  # -- API interface used by the object store. Supported values include `s3`, 's3_compatible'
  # and `gcs`. For pod mapped storage, use 'local'
  storage: s3

  # -- Optional properties for storage. Supported values depend on the type of the storage.
  storage_properties: {}
    # Define region for s3 / s3_compatible
    # region: eu-west-1

    # For s3_compatible option, one must define target host
    # host: 192.168.1.201

    # For s3_compatible option, port is optional
    # port: 9000

  # Is SSL used or not for s3_compatible connection
  # secure: false

  # -- Name of the remote storage bucket where backups will be stored. If using 'local' storage, set the
  # path of the local storage here.
  bucketName: awstest

  # -- Name of the Kubernetes `Secret` that stores the key file for the
  # storage provider's API
  storageSecret: medusa-bucket-key

monitoring:
  grafana:
    # -- Enables the creation of configmaps containing Grafana dashboards. If
    # leveraging the kube prometheus stack sub-chart this value should be
    # `true`.
    provision_dashboards: true

  prometheus:
    # -- Enables the creation of Prometheus Operator ServiceMonitor custom
    # resources. If you are not using the kube prometheus stack sub-chart or do
    # not have the ServiceMonitor CRD installed on your cluster, set this value
    # to `false`.
    provision_service_monitors: true

cleaner:
  image: k8ssandra/k8ssandra-cleaner:618b8ff9d368

cass-operator:
  # -- Enables the cass-operator as part of this release. If this setting is
  # disabled no Cassandra resources will be deployed.
  enabled: true

reaper-operator:
  # -- Enables the reaper-operator as part of this release. If this setting is
  # disabled no repair resources will be deployed.
  enabled: true

# Configuration values for the kube-prometheus-stack chart. Not all values are
# provided here for an exhaustive list see:
# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
kube-prometheus-stack:
  # -- Controls whether the kube-prometheus-stack chart is used at all.
  # Disabling this parameter prevents all monitoring components from being
  # installed.
  enabled: true

  # Disable default service monitors
  coreDns:
    enabled: false
  kubeApiServer:
    enabled: false
  kubeControllerManager:
    enabled: false
  kubeDns:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeProxy:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeStateMetrics:
    enabled: false
  kubelet:
    enabled: false
  nodeExporter:
    enabled: false

  alertmanager:
    # Disabled for now while we build out a set of default alerts
    enabled: false
    serviceMonitor:
      selfMonitor: false

  prometheusOperator:
    # Installs the Prometheus Operator, omitting this parameter will result in
    # resources not being deployed.
    enabled: true

    # -- Locks Prometheus operator to this namespace. Changing this setting may
    # result in a non-namespace scoped deployment.
    namespaces:
      releaseNamespace: true
      additional: []

    # -- Monitoring of prometheus operator
    serviceMonitor:
      selfMonitor: false

  prometheus:
    # -- Provisions an instance of Prometheus as part of this release
    enabled: true

    # -- Allows for tweaking of the Prometheus installation's configuration.
    # Common parameters include `externalUrl: http://localhost:9090/prometheus`
    # and `routePrefix: /prometheus` for running Prometheus resources under a
    # specific path (`/prometheus` in this example).
    prometheusSpec:
      # -- Prefixes all Prometheus routes with the specified value. It is useful
      # for ingresses which do not rewrite URLs.
      routePrefix: /

      # -- An external URL at which Prometheus will be reachable.
      externalUrl: ""

    ingress:
      # -- Enable templating of ingress resources for external prometheus
      # traffic
      enabled: false
      # -- Path-based routing rules, `/prometheus` is possible if the
      # appropriate changes are made to `prometheusSpec`
      paths: []

    serviceMonitor:
      # Disable monitoring the Prometheus instance
      selfMonitor: false

  grafana:
    # -- Provisions an instance of Grafana and wires it up with a DataSource
    # referencing this Prometheus installation
    enabled: true
    ingress:
      # -- Generates ingress resources for the Grafana instance
      enabled: false

      # -- Path-based routing rules, '/grafana' is possible if appropriate
      # changes are made to `grafana.ini`
      path:

    # -- Username for accessing the provisioned Grafana instance
    adminUser: admin

    # -- Password for accessing the provisioned Grafana instance
    adminPassword: secret

    serviceMonitor:
      # -- Whether the Grafana instance should be monitored
      selfMonitor: false

    # -- Default dashboard installation
    defaultDashboardsEnabled: false

    # -- Additional plugins to be installed during Grafana startup,
    # `grafana-polystat-panel` is used by the default Cassandra dashboards.
    plugins:
      - grafana-polystat-panel

    # -- Customization of the Grafana instance. To listen for Grafana traffic
    # under a different url set `server.root_url: http://localhost:3000/grafana`
    # and `serve_from_sub_path: true`.
    grafana.ini: {}
